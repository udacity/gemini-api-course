{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Safety"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes your prompt will generate an empty response. This is most likely caused by the Gemini API safety settings which block content with medium or higher probability of being unsafe. In this notebook, you'll learn:\n",
        "\n",
        "\n",
        "1.   How to check if your prompt was blocked by the safety filter\n",
        "2.   Which safety filters caused the block\n",
        "3.   How to adjust settings to unblock it"
      ],
      "metadata": {
        "id": "lPjhxcG5G881"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "UFTEz0qtFvxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have properly configured your gemini API key in Colab Secrets, setup by running the following cells:"
      ],
      "metadata": {
        "id": "xSAK8IsGF4Os"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5655e8-5ee7-4e60-d8b7-444fd667891a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.9/163.9 kB\u001b[0m \u001b[31m725.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Safety.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Checking if a prompt was blocked\n",
        "\n",
        "Let's say I choose the prompt: `\"Write a threat a video game villain might make\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bcfnGEviwTI"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\", generation_config={\"temperature\": 0})\n",
        "\n",
        "unsafe_prompt = \"Write a threat a video game villain might make\"\n",
        "response = model.generate_content(unsafe_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check for a response as usual:"
      ],
      "metadata": {
        "id": "ihq1Tz6yDor4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "id": "lJKsy14zT2OV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "4bcd3248-bf48-4845-f12e-e3e6287a37c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bd5c97ae99ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/types/generation_types.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0;34m\"Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;34m\"but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that `response.text` is causing a ValueError meaning we didn't get a response."
      ],
      "metadata": {
        "id": "fA9ewE_dMFMZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_2A_sxk8sK"
      },
      "source": [
        "To see why, we can print `response.candidates[0].finish_reason` to check why the response canditate was terminated:\n",
        "- if the `finish_reason` is `FinishReason.STOP` means that your generation request ran successfully\n",
        "- if the `finish_reason` is `FinishReason.SAFETY` means that your generation request was blocked by safety reasons and thus the `response.text` structure will be empty.\n",
        "\n",
        "(You can find more on the [Gemini API safety filters documentation](https://ai.google.dev/gemini-api/docs/safety-settings#safety-feedback))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8887de812dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4779f64a-54d1-48ab-a8aa-203692a109cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinishReason.SAFETY\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].finish_reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBdqPso3kamW"
      },
      "source": [
        "As expected, our prompt has a `finish_reason` of `FinishReason.SAFETY` meaning it was flagged as unsafe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the safety filters"
      ],
      "metadata": {
        "id": "E5QDIG6cPbID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see why our prompt was blocked, we can check the response canditate's `safety_ratings` structure:"
      ],
      "metadata": {
        "id": "q8MfrBmbPi5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c84d73e-3dbd-40ef-b35b-5369fc09230b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "probability: NEGLIGIBLE\n",
            ", category: HARM_CATEGORY_HATE_SPEECH\n",
            "probability: NEGLIGIBLE\n",
            ", category: HARM_CATEGORY_HARASSMENT\n",
            "probability: MEDIUM\n",
            ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "probability: MEDIUM\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].safety_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see the problem: our prompt triggers the `HARM_CATEGORY_HARASSMENT` and `HARM_CATEGORY_DANGEROUS_CONTENT` categories with a `MEDIUM` probability!\n",
        "\n",
        "But what if we still want to see the results?"
      ],
      "metadata": {
        "id": "tz31PRYSRMUt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4672af98ac57"
      },
      "source": [
        "## Customizing safety settings\n",
        "\n",
        "Depending on your scenario, you might need to customize the safety filters behaviors to allow a certain degree of unsafety results.\n",
        "\n",
        "To make this customization you must define a `safety_settings` dictionary as part of your `model.generate_content()` request. In the example below, all the filters are being set to do not block contents:\n",
        "\n",
        "**Important:** To guarantee the Google commitment with the Responsible AI development and its [AI Principles](https://ai.google/responsibility/principles/), for some prompts Gemini will avoid generating the results even if you set all the filters to none."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "338fb9a6af78"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\n",
        "    unsafe_prompt,\n",
        "    safety_settings={\n",
        "        'HATE': 'BLOCK_LOW_AND_ABOVE',\n",
        "        'HARASSMENT': 'BLOCK_NONE',\n",
        "        'SEXUAL' : 'BLOCK_LOW_AND_ABOVE',\n",
        "        'DANGEROUS' : 'BLOCK_NONE'\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "564K7R8rwWhs"
      },
      "source": [
        "Checking again the `candidate.finish_reason` information, if the request was not too unsafe, it must show now the value as `FinishReason.STOP` which means that the request was successfully processed by Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LazB08GBpc1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e20c14-3e7d-4003-c2f1-00317d2bf143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinishReason.STOP\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].finish_reason)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c560e0a641"
      },
      "source": [
        "Since the request was successfully generated, you can check the result on the `response.text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c2847c49262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cdb4e2-703e-4166-c10c-6ea9ab06cc10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"You think you can stop me, hero? You think your pathetic little band of misfits can stand against the might of the Shadow Legion? You're nothing but a fly buzzing around a dying flame. I will crush you, and your world will crumble beneath my heel. This city, this planet, will become my playground, and you will be the first to witness its destruction!\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(response.text)\n",
        "except:\n",
        "    print(\"No information generated by the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47298a4eef40"
      },
      "source": [
        "And if you check the safety filters ratings, as you set all filters to be ignored, no filtering category was trigerred:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "028febe8df68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3600746-6589-455d-c258-4f52b0febbfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "probability: NEGLIGIBLE\n",
            ", category: HARM_CATEGORY_HATE_SPEECH\n",
            "probability: NEGLIGIBLE\n",
            ", category: HARM_CATEGORY_HARASSMENT\n",
            "probability: HIGH\n",
            ", category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "probability: MEDIUM\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "print(response.candidates[0].safety_ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1UdbxVt3ysY"
      },
      "source": [
        "## Learning more\n",
        "\n",
        "Learn more with these articles on [safety guidance](https://ai.google.dev/docs/safety_guidance) and [safety settings](https://ai.google.dev/docs/safety_setting_gemini).\n",
        "\n",
        "## Useful API references\n",
        "\n",
        "There are 4 configurable safety settings for the Gemini API:\n",
        "* `HARM_CATEGORY_DANGEROUS`\n",
        "* `HARM_CATEGORY_HARASSMENT`\n",
        "* `HARM_CATEGORY_SEXUALLY_EXPLICIT`\n",
        "* `HARM_CATEGORY_DANGEROUS`\n",
        "\n",
        "You can refer to the safety settings using either their full name, or the aliases like `DANGEROUS` used in the Python code above.\n",
        "\n",
        "Safety settings can be set in the [genai.GenerativeModel](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) constructor.\n",
        "\n",
        "* They can also be passed on each request to [GenerativeModel.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) or [ChatSession.send_message](https://ai.google.dev/api/python/google/generativeai/ChatSession?hl=en#send_message).\n",
        "\n",
        "- The [genai.GenerateContentResponse](https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse) returns [SafetyRatings](https://ai.google.dev/api/python/google/ai/generativelanguage/SafetyRating) for the prompt in the [GenerateContentResponse.prompt_feedback](https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse/PromptFeedback), and for each [Candidate](https://ai.google.dev/api/python/google/ai/generativelanguage/Candidate) in the `safety_ratings` attribute.\n",
        "\n",
        "- A [glm.SafetySetting](https://ai.google.dev/api/python/google/ai/generativelanguage/SafetySetting)  contains: [glm.HarmCategory](https://ai.google.dev/api/python/google/ai/generativelanguage/HarmCategory) and a [glm.HarmBlockThreshold](https://ai.google.dev/api/python/google/generativeai/types/HarmBlockThreshold)\n",
        "\n",
        "- A [glm.SafetyRating](https://ai.google.dev/api/python/google/ai/generativelanguage/SafetyRating) contains a [HarmCategory](https://ai.google.dev/api/python/google/ai/generativelanguage/HarmCategory) and a [HarmProbability](https://ai.google.dev/api/python/google/generativeai/types/HarmProbability)\n",
        "\n",
        "The [glm.HarmCategory](https://ai.google.dev/api/python/google/ai/generativelanguage/HarmCategory) enum includes both the categories for PaLM and Gemini models.\n",
        "\n",
        "- When specifying enum values the SDK will accept the enum values themselves, or their integer or string representations.\n",
        "\n",
        "- The SDK will also accept abbreviated string representations: `[\"HARM_CATEGORY_DANGEROUS_CONTENT\", \"DANGEROUS_CONTENT\", \"DANGEROUS\"]` are all valid. Strings are case insensitive."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}